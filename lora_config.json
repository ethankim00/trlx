{
    "model": {
        "num_layers_unfrozen": -1,
        "model_path": "EleutherAI/pythia-6.9B",
        "delta_kwargs": {
            "lora_r": 8,
            "lora_alpha": 16,
            "delta_type": "lora"
        }
    },
    "train": {
        "batch_size": 16
    },
    "tokenizer": {
        "tokenizer_path": "EleutherAI/pythia-6.9B"
    },
    "method": {
        "chunk_size": 32
    }
}